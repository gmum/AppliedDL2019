{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvNets_answers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "tZHE9NDW4N8R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UsM9AH7a-ppu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4m3uGScHLCN6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load CIFAR10 dataset"
      ]
    },
    {
      "metadata": {
        "id": "iJn8yNvQO4Ex",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://appliedmachinelearning.files.wordpress.com/2018/03/cifar2.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "v7B88f0w-psd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cifar = keras.datasets.cifar10\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar.load_data()\n",
        "\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "enc = OneHotEncoder(sparse=False, categories='auto')\n",
        "\n",
        "y_train = enc.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test = enc.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6hmn0bwK-pu0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ty8m_Rl_LEoH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create Convolutional Neural Networks for CIFAR images classification\n"
      ]
    },
    {
      "metadata": {
        "id": "ly_v7k_BONQP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What convolutional neural network is?"
      ]
    },
    {
      "metadata": {
        "id": "w4_ydmzOOGAs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://cdn-images-1.medium.com/max/1400/1*vkQ0hXDaQv57sALXAJquxA.jpeg)"
      ]
    },
    {
      "metadata": {
        "id": "gzKUKG7fImqD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What is the image filter"
      ]
    },
    {
      "metadata": {
        "id": "Ftf2rLuxItBM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://www.researchgate.net/profile/Seiichi_Uchida/publication/236125496/figure/fig5/AS:214053629763589@1428045770840/Effect-of-three-different-edge-detection-filters-Laplacian-Canny-and-Sobel-filters.png)"
      ]
    },
    {
      "metadata": {
        "id": "SnQ4VrUbOetp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What is the convolution operation?"
      ]
    },
    {
      "metadata": {
        "id": "iR53l1urOLHC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://cdn-images-1.medium.com/max/800/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif)"
      ]
    },
    {
      "metadata": {
        "id": "__K9g1WOjwWW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://cdn-images-1.medium.com/max/800/1*ciDgQEjViWLnCbmX-EeSrA.gif)"
      ]
    },
    {
      "metadata": {
        "id": "yejEDCzLJz7k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What is the pooling operation"
      ]
    },
    {
      "metadata": {
        "id": "LZ5Dt6dDJ0L6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://cdn-images-1.medium.com/max/600/1*uoWYsCV5vBU8SHFPAPao-w.gif)"
      ]
    },
    {
      "metadata": {
        "id": "--6ge37mNUSJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the network\n",
        "\n",
        "In the following section we will use the tensorflow lower level API to define the convolutional neural network that will classify CIFAR10 images and all auxillary functions that will be useful during training."
      ]
    },
    {
      "metadata": {
        "id": "kJ1qr9JuQAAr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create placeholders for training data.**\n",
        "\n",
        "Remember about a propper shape of training images (every training example is an image shaped 32x32x3 pixels) and labels (In training dataset labels are in one-hot-encoding form)."
      ]
    },
    {
      "metadata": {
        "id": "lDXwhkQM-pxw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_placeholder = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
        "y_placeholder = tf.placeholder(tf.float32, [None, 10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MvWNjvizQVqN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Create ConvNet **\n",
        "\n",
        "Define the following layers of your new model:\n",
        "\n",
        "1.   Convolutional layer [keras.layers.Conv2d](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) that will process the input data (feed by placeholder). The layer should have 32 filters, with kernel size equal to 3, same padding and ReLU activation function.\n",
        "2.   Another convolutional layer with 32 filters, kernel size equal to 3, same padding and ReLU activation function.\n",
        "3.   Max pooling layer [keras.layers.MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D), with pool size equal to 2.\n",
        "4.   Convolutional layer with 64 filters, kernel size equal to 3, same padding and ReLU activation function.\n",
        "5.   Another convolutional layer with 64 filters, kernel size equal to 3, same padding and ReLU activation function.\n",
        "6.   Max pooling layer with pool size equal to 2.\n",
        "7.   Layer that will flatten the result of convolutions [keras.layers.Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten).\n",
        "8.   Dense layer ([keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) with 128 neurons (aka units), and the \"relu\" activation function.\n",
        "4.  A final Dense layer with 10 neurons (one per class), , without any activation function -- returning the logits (not softmax), together with the specified loss function ensures the numerical stability."
      ]
    },
    {
      "metadata": {
        "id": "wkXBP1ZW-p01",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "conv1 = keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\")(x_placeholder)\n",
        "conv2 = keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\")(conv1)\n",
        "pool1 = keras.layers.MaxPool2D(pool_size=2)(conv2)\n",
        "conv3 = keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\")(pool1)\n",
        "conv4 = keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\")(conv3)\n",
        "pool2 = keras.layers.MaxPool2D(pool_size=2)(conv4)\n",
        "flatten = keras.layers.Flatten()(pool2)\n",
        "dense = keras.layers.Dense(128, activation=\"relu\")(flatten)\n",
        "logits = keras.layers.Dense(10, activation=None)(dense)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5x5_kXm-SFy0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Define the loss function**\n",
        "\n",
        "This is the place to define the loss function for our model. Cross-entropy is the classical approach to use in the multi-label classification task.  However for numerical stability we didn't use the softmax activation function (so as we are taking the log of softmax -- logits).\n",
        "\n",
        "This time, instead of using [tf.nn.softmax_cross_entropy_with_logits_v2](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2), you could use the [softmax_cross_entropy](https://www.tensorflow.org/api_docs/python/tf/losses/softmax_cross_entropy) function from [tf.losses](https://www.tensorflow.org/api_docs/python/tf/losses) module, which is the module with some predefined loss functions."
      ]
    },
    {
      "metadata": {
        "id": "rB0QACjm-p3q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.losses.softmax_cross_entropy(onehot_labels=y_placeholder, logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BQC2rmCDTKex",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Define the optimizer**\n",
        "\n",
        "Use the [Adam optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) and set it to minimize the loss function."
      ]
    },
    {
      "metadata": {
        "id": "QUMZ1Bqa-p64",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U0iJOOiFTZPF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Define operation for accuracy calculations **\n",
        "\n",
        "This time instead od writing it by ourselves, we will use the [accuracy](https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy) function from [tf.metrics](https://www.tensorflow.org/api_docs/python/tf/metrics) module, which is the module with some predefined metrics for validation of neural networks.\n",
        "\n",
        "We cannot test our network on the whole test set, as it won't fit in the memory of our GPU. Instead we could calculate our metrics in batches. In order to do that, remember about *update_op* returned by accuracy function, [like in the following link](http://ronny.rest/blog/post_2017_09_11_tf_metrics/)."
      ]
    },
    {
      "metadata": {
        "id": "61q8X3mEKlnu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "accuracy, update_op = tf.metrics.accuracy(labels=tf.argmax(y_placeholder, 1), predictions=tf.argmax(logits, 1), name=\"accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "syAiUKvxTTEt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Isolate the variables stored behind the scenes by the metric operation\n",
        "running_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"accuracy\")\n",
        "\n",
        "# Define initializer to initialize/reset running variables\n",
        "running_vars_initializer = tf.variables_initializer(var_list=running_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AVUCXYzwUZ1e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Define the function that calculates the accuracy in batches **"
      ]
    },
    {
      "metadata": {
        "id": "SHJTHmSpRKXt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calculate_accuracy_batches(X_test, y_test, batch_size, accuracy, update_op, sess):    \n",
        "    for i in range(0, X_test.shape[0], batch_size):\n",
        "        y_batch = y_test[i:(i + batch_size), :]\n",
        "        x_batch = X_test[i:(i + batch_size), :]\n",
        "        sess.run(update_op, feed_dict={\n",
        "            x_placeholder: x_batch, y_placeholder: y_batch})\n",
        "        \n",
        "    return sess.run(accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8WIl4dj4S5Et",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hOypevsDS5PX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the network"
      ]
    },
    {
      "metadata": {
        "id": "HPs_QiA1Klq4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epoch_num = 10\n",
        "batch_size = 128\n",
        "set_size = X_train.shape[0]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(running_vars_initializer)\n",
        "    \n",
        "    validation_accuracy = calculate_accuracy_batches(X_test, y_test, batch_size,\n",
        "                                                     accuracy, update_op, sess)\n",
        "    print('Validation accuracy before training: {}'.format(validation_accuracy))\n",
        "    \n",
        "    for epoch in range(epoch_num):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        perm = np.random.permutation(set_size)\n",
        "        X_train = X_train[perm, :]\n",
        "        y_train = y_train[perm, :]\n",
        "\n",
        "        for i in range(0, set_size, batch_size):\n",
        "            step_size = min(batch_size, set_size - i)\n",
        "\n",
        "            if step_size > 1:\n",
        "                y_batch = y_train[i:(i + step_size), :]\n",
        "                x_batch = X_train[i:(i + step_size), :]\n",
        "                train_step.run(feed_dict={\n",
        "                                x_placeholder: x_batch, y_placeholder: y_batch})\n",
        "                \n",
        "        validation_accuracy = calculate_accuracy_batches(X_test, y_test, \n",
        "                                          batch_size, accuracy, update_op, sess)\n",
        "        end_time = time.time()\n",
        "        print('step: {}, validation accuracy: {}, epoch time: {}'.format(epoch, validation_accuracy, end_time-start_time))\n",
        "\n",
        "\n",
        "    # Print the test set accuracy\n",
        "    test_accuracy = calculate_accuracy_batches(X_test, y_test, batch_size, accuracy, update_op, sess)\n",
        "    print('test accuracy: {}'.format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xVA6Rp4hL1D2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2WlxX3-cL1RJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TensorFlow Datasets"
      ]
    },
    {
      "metadata": {
        "id": "0HwgPadjiuG8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "GPUs and TPUs can radically reduce the time required to execute a single training step. Achieving peak performance requires an efficient input pipeline that delivers data for the next step before the current step has finished. The tf.data API helps to build flexible and efficient input pipelines.\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "A typical TensorFlow training input pipeline can be framed as an ETL process:\n",
        "\n",
        "1.    Extract: Read data from persistent storage -- either local (e.g. HDD or SSD) or remote (e.g. GCS or HDFS).\n",
        "2.    Transform: Use CPU cores to parse and perform preprocessing operations on the data such as image decompression, data augmentation transformations (such as random crop, flips, and color distortions), shuffling, and batching.\n",
        "3.    Load: Load the transformed data onto the accelerator device(s) (for example, GPU(s) or TPU(s)) that execute the machine learning model.\n",
        "\n",
        "This pattern effectively utilizes the CPU, while reserving the accelerator for the heavy lifting of training your model. In addition, viewing input pipelines as an ETL process provides structure that facilitates the application of performance optimizations."
      ]
    },
    {
      "metadata": {
        "id": "h7todTTWjBk0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**In a naive *feed_dict* pipeline the GPU always sits by idly whenever it has to wait for the CPU to provide it with the next batch of data.**\n",
        "\n",
        "![](https://dominikschmidt.xyz/tensorflow-data-pipeline/assets/feed_dict_pipeline.png)"
      ]
    },
    {
      "metadata": {
        "id": "cN5eTxtRjIbD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**A *tf.data* pipeline, however, can prefetch the next batches asynchronously to minimize the total idle time. You can further speed up the pipeline by parallelizing the loading and preprocessing operations.**\n",
        "![](https://dominikschmidt.xyz/tensorflow-data-pipeline/assets/tf_data_pipeline.png)"
      ]
    },
    {
      "metadata": {
        "id": "V1eyntlmk73U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Code examples\n",
        "\n",
        "More examples on [datasets guide](https://www.tensorflow.org/guide/datasets)"
      ]
    },
    {
      "metadata": {
        "id": "bALY8VI6lYOb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Dataset with one-shot iterator**\n",
        "\n",
        "A one-shot iterator is the simplest form of iterator, which only supports iterating once through a dataset, with no need for explicit initialization."
      ]
    },
    {
      "metadata": {
        "id": "Seu6fnQYL5Pf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    dataset = tf.data.Dataset.range(100)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    next_element = iterator.get_next()\n",
        "\n",
        "    for i in range(100):\n",
        "        value = sess.run(next_element)\n",
        "        assert i == value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pOTfZr4Ulx6S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Dataset with initializable iterator **\n",
        "\n",
        "An initializable iterator requires you to run an explicit iterator.initializer operation before using it. In exchange for this inconvenience, it enables you to parameterize the definition of the dataset, using one or more tf.placeholder() tensors that can be fed when you initialize the iterator."
      ]
    },
    {
      "metadata": {
        "id": "C9ng5jdIlUei",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    max_value = tf.placeholder(tf.int64, shape=[])\n",
        "    dataset = tf.data.Dataset.range(max_value)\n",
        "    iterator = dataset.make_initializable_iterator()\n",
        "    next_element = iterator.get_next()\n",
        "\n",
        "    # Initialize an iterator over a dataset with 10 elements.\n",
        "    sess.run(iterator.initializer, feed_dict={max_value: 10})\n",
        "    for i in range(10):\n",
        "        value = sess.run(next_element)\n",
        "        assert i == value\n",
        "\n",
        "    # Initialize the same iterator over a dataset with 100 elements.\n",
        "    sess.run(iterator.initializer, feed_dict={max_value: 100})\n",
        "    for i in range(100):\n",
        "        value = sess.run(next_element)\n",
        "        assert i == value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xHit3tiql-TL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** End of dataset **\n",
        "\n",
        "If the iterator reaches the end of the dataset, executing the Iterator.get_next() operation will raise a tf.errors.OutOfRangeError. After this point the iterator will be in an unusable state, and you must initialize it again if you want to use it further."
      ]
    },
    {
      "metadata": {
        "id": "fB-fQF1BlUsv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    dataset = tf.data.Dataset.range(5)\n",
        "    iterator = dataset.make_initializable_iterator()\n",
        "    next_element = iterator.get_next()\n",
        "\n",
        "    # Typically `result` will be the output of a model, or an optimizer's\n",
        "    # training operation.\n",
        "    result = tf.add(next_element, next_element)\n",
        "\n",
        "    sess.run(iterator.initializer)\n",
        "    print(sess.run(result))  # ==> \"0\"\n",
        "    print(sess.run(result))  # ==> \"2\"\n",
        "    print(sess.run(result))  # ==> \"4\"\n",
        "    print(sess.run(result))  # ==> \"6\"\n",
        "    print(sess.run(result))  # ==> \"8\"\n",
        "    \n",
        "    try:\n",
        "        sess.run(result)\n",
        "    except tf.errors.OutOfRangeError:\n",
        "        print(\"End of dataset\")  # ==> \"End of dataset\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4eq40VJWqizS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Preprocessing data with Dataset.map()**\n",
        "\n",
        "The Dataset.map(f) transformation produces a new dataset by applying a given function f to each element of the input dataset. It is based on the map() function that is commonly applied to lists (and other structures) in functional programming languages. The function f takes the tf.Tensor objects that represent a single element in the input, and returns the tf.Tensor objects that will represent a single element in the new dataset. Its implementation uses standard TensorFlow operations to transform one element into another."
      ]
    },
    {
      "metadata": {
        "id": "p5OnT3O2p2SP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parse_function(value):\n",
        "    return value * value\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    dataset = tf.data.Dataset.range(10)\n",
        "    dataset = dataset.map(map_func=parse_function, num_parallel_calls=2)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    next_element = iterator.get_next()        \n",
        "        \n",
        "    try:\n",
        "        while True:\n",
        "            value = sess.run(next_element)\n",
        "            print(value)\n",
        "    except tf.errors.OutOfRangeError:\n",
        "        print(\"End of dataset\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HPhdMM0lmCZk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KCkL19XvL-LB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Use Dataset for ConvNet training\n"
      ]
    },
    {
      "metadata": {
        "id": "Hege2bkYUwW6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following section, we will replace manual batch preparation by the [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) pipeline.\n",
        "\n",
        "Our model is already defined, so that we will only prepare the training pipeline."
      ]
    },
    {
      "metadata": {
        "id": "GynIbLy7luB2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epoch_num = 10\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "707xjhsvVqJQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If all of your input data fit in memory, the simplest way to create a Dataset from them is to convert them to tf.Tensor objects and use **Dataset.from_tensor_slices()**.\n",
        "\n",
        "However doing so will embed the features and labels arrays in your TensorFlow graph as tf.constant() operations. This works well for a small dataset, but wastes memory---because the contents of the array will be copied multiple times---and can run into the 2GB limit for the tf.GraphDef protocol buffer.\n",
        "\n",
        "As an alternative, you can define the Dataset in terms of **tf.placeholder()** tensors, [like in the following link](https://www.tensorflow.org/guide/datasets#consuming_numpy_arrays), and feed the NumPy arrays when you initialize an Iterator over the dataset."
      ]
    },
    {
      "metadata": {
        "id": "6lwSnXQYbRd4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the tf dataset"
      ]
    },
    {
      "metadata": {
        "id": "8oI6CxD2XiFK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create placeholders for dataset features and labels.**\n",
        "\n",
        "Remember about a propper shape and type. One placeholder will be feeded with images and another with one-hot encodings."
      ]
    },
    {
      "metadata": {
        "id": "CEe9xfweNVSI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features_placeholder = tf.placeholder(x_placeholder.dtype, x_placeholder.shape)\n",
        "labels_placeholder = tf.placeholder(y_placeholder.dtype, y_placeholder.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3_JghNuoX-Sk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Define the dataset from placeholders **"
      ]
    },
    {
      "metadata": {
        "id": "SmLhgI27WSZt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rBLrbxmaY22A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Define the dataset structure **\n",
        "\n",
        "We will use the dataset to train our model, so that we want to **shuffle** our data before every epoch, we also want to train our model in **batches**.  Setting [prefetch](https://www.tensorflow.org/guide/performance/datasets#pipelining) buffer could be also useful to spped up our training."
      ]
    },
    {
      "metadata": {
        "id": "8SlYq2pJT-1R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(buffer_size=50000)\n",
        "dataset = dataset.batch(batch_size)\n",
        "dataset = dataset.prefetch(buffer_size=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zQDmAg71arRJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Create an iterator **\n",
        "\n",
        "Create the [initializable iterator](https://www.tensorflow.org/guide/datasets#creating_an_iterator) and get its symbolic output."
      ]
    },
    {
      "metadata": {
        "id": "t6nqAQHuU_l-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "iterator = dataset.make_initializable_iterator()\n",
        "next_example, next_label = iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "COXclbgpbEvB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Define the function that calculates the accuracy in batches and uses the created dataset**"
      ]
    },
    {
      "metadata": {
        "id": "P6hz3iyVkCeE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calculate_accuracy_batches_dataset(X_test, y_test, iterator, accuracy, update_op, sess):\n",
        "    sess.run(iterator.initializer, feed_dict={features_placeholder: X_test,\n",
        "                                              labels_placeholder: y_test})\n",
        "    \n",
        "    try:\n",
        "        while True:\n",
        "            x_batch, y_batch = sess.run([next_example, next_label])\n",
        "            sess.run(update_op, feed_dict={x_placeholder: x_batch, y_placeholder: y_batch})\n",
        "    except tf.errors.OutOfRangeError:\n",
        "        pass\n",
        "        \n",
        "    return sess.run(accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SUFqxWblbWeQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the network"
      ]
    },
    {
      "metadata": {
        "id": "y4zL0Uft-p9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(running_vars_initializer)\n",
        "    \n",
        "    validation_accuracy = calculate_accuracy_batches_dataset(X_test, y_test, iterator, accuracy, update_op, sess)\n",
        "    print('Validation accuracy before training: {}'.format(validation_accuracy))\n",
        "    \n",
        "    for epoch in range(epoch_num):\n",
        "        start_time = time.time()\n",
        "        sess.run(iterator.initializer, feed_dict={features_placeholder: X_train,\n",
        "                                                  labels_placeholder: y_train})\n",
        "        \n",
        "        try:\n",
        "            while True:\n",
        "                x_batch, y_batch = sess.run([next_example, next_label])\n",
        "                train_step.run(feed_dict={x_placeholder: x_batch, y_placeholder: y_batch})\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "        \n",
        "        validation_accuracy = calculate_accuracy_batches_dataset(X_test, y_test, iterator, accuracy, update_op, sess)\n",
        "        end_time = time.time()\n",
        "        print('step: {}, validation accuracy: {}, epoch time: {}'.format(epoch, validation_accuracy, end_time-start_time))\n",
        "        \n",
        "    # Print the test set accuracy\n",
        "    test_accuracy = calculate_accuracy_batches_dataset(X_test, y_test, iterator, accuracy, update_op, sess)\n",
        "    print('test accuracy: {}'.format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "86mv-VHafE1j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note: You can also use the tf dataset to train model in keras sequential or keras functional API [link](https://stackoverflow.com/questions/52736517/tensorflow-keras-with-tf-dataset-input)**"
      ]
    },
    {
      "metadata": {
        "id": "2u99hkMAnOMH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UutEAcwEnPef",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Images augmentation"
      ]
    },
    {
      "metadata": {
        "id": "lC0-3f-vmkjy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Data augmentation can artificially increase the size of our training datasets and therefore increase the metrics of our neural networks.\n",
        "\n",
        "\\\\\n",
        "\n",
        "In TensorFlow images augmentation could be computed on CPU and on GPU. Images augmentation operations are defined in [tf.image](https://www.tensorflow.org/api_docs/python/tf/image) module."
      ]
    },
    {
      "metadata": {
        "id": "2FP3XoEvnNpS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Some images augmentation examples"
      ]
    },
    {
      "metadata": {
        "id": "SJQAe3_-nUN4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Original image**\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*EqjJ1tEsRoc4FNptI9teeA.jpeg)"
      ]
    },
    {
      "metadata": {
        "id": "di-8fuTKneUw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Flipped images**\n",
        "\n",
        "images flipping could be done by using [tf.image.random_flip_left_right](https://www.tensorflow.org/api_docs/python/tf/image/random_flip_left_right) and [tf.image.random_flip_up_down](https://www.tensorflow.org/api_docs/python/tf/image/random_flip_up_down) functions\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*UYDREwlBU0ua9-g5ccRWgQ.png)\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*NmIPTEmki86rAlDkqOPHiQ.png)"
      ]
    },
    {
      "metadata": {
        "id": "jXYAdbNQoQT_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Images rotation **\n",
        "\n",
        "Which could be done by using [tf.image.rot90](https://www.tensorflow.org/api_docs/python/tf/image/rot90) and [tf.contrib.image.rotate](https://www.tensorflow.org/api_docs/python/tf/contrib/image/rotate) functions. \n",
        "\n",
        "**Note:** Warning, tf.contrib will be deleted in tf 2.0\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*xtZM7snC-6Nkq4_iB1RnKg.png)"
      ]
    },
    {
      "metadata": {
        "id": "RC4nRY8Znmq1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D0HFlMdynm7N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ConvNet training with images augmentation"
      ]
    },
    {
      "metadata": {
        "id": "KoubiFhgbyhD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following section we will define some image augmentations that will be proceeded by our dataset on CPU, in the meantime when batch gradient will be computed on the GPU."
      ]
    },
    {
      "metadata": {
        "id": "Y3A4mPiCgYBz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the augmentation and tf dataset"
      ]
    },
    {
      "metadata": {
        "id": "w0XHK2zgfl_Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Define the augmentation function**\n",
        "\n",
        "Define the augmentation function with using of [tf.image](https://www.tensorflow.org/api_docs/python/tf/image) module. You could use any augmentation that you like.\n",
        "\n",
        "Please notice that since the pipeline parsing function should be running on CPU, we have to specify the [device](https://www.tensorflow.org/api_docs/python/tf/device) manually."
      ]
    },
    {
      "metadata": {
        "id": "EVQWd9t0nGJu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cpu_augmentation_fn(image, label):\n",
        "    with tf.device('/cpu:0'):\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "        image = tf.image.random_flip_up_down(image)\n",
        "        image = tf.cond(\n",
        "            tf.less(tf.random_uniform(()), 0.5),\n",
        "            lambda: tf.contrib.image.rotate(\n",
        "                image,\n",
        "                tf.random_uniform((), -30 / 180 * math.pi, 30 / 180 * math.pi)\n",
        "            ),\n",
        "            lambda: tf.identity(image)\n",
        "        )\n",
        "\n",
        "    return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m-prhSvag4_m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Define the dataset **\n",
        "\n",
        "We will use the dataset to train our model, so that we want to **shuffle** our data before every epoch, we also want to train our model in **batches**.  Setting [prefetch](https://www.tensorflow.org/guide/performance/datasets#pipelining) buffer could be also useful to spped up our training. We also want to use our predefined augmentation as the dataset [parsing function](https://www.tensorflow.org/guide/datasets#preprocessing_data_with_datasetmap).\n",
        "\n",
        "**Note:** Please notice that we don't want to make an augmentation during the testing of our network. Not shuffling during test time is also a good idea.\n",
        "\n",
        "To handle this problem, we will define the function that creates the dataset with a given specified mode: *train/test*. And then, by calling this function two times, we will define the train and test datasets."
      ]
    },
    {
      "metadata": {
        "id": "XZZnh_veoW-N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_dataset(mode='train', parse_fn=cpu_augmentation_fn, batch_size=128, buffer_size=50000, channels=3):\n",
        "    features_placeholder = tf.placeholder(x_placeholder.dtype, x_placeholder.shape)\n",
        "    labels_placeholder = tf.placeholder(y_placeholder.dtype, y_placeholder.shape)\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
        "    \n",
        "    if mode == 'train':\n",
        "        dataset = dataset.map(map_func=parse_fn, num_parallel_calls=8)\n",
        "        dataset = dataset.shuffle(buffer_size=buffer_size)\n",
        "        \n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size=5)\n",
        "    \n",
        "    return dataset, features_placeholder, labels_placeholder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6pCd7tzlzv_Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset, train_features_placeholder, train_labels_placeholder = create_dataset(mode='train')\n",
        "val_dataset, val_features_placeholder, val_labels_placeholder = create_dataset(mode='valid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5K4WG_3Dh_Ce",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Create an iterator **\n",
        "\n",
        "Create the [initializable iterator](https://www.tensorflow.org/guide/datasets#creating_an_iterator) and get its symbolic output for both training and testing datasets."
      ]
    },
    {
      "metadata": {
        "id": "xTBey4_jzv8x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_iterator = train_dataset.make_initializable_iterator()\n",
        "next_train_example, next_train_label = train_iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GT2oILPg0Alj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_iterator = val_dataset.make_initializable_iterator()\n",
        "next_val_example, next_val_label = val_iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aD8xfpw3iLoX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Define the function that calculates the accuracy in batches and uses the created test dataset**"
      ]
    },
    {
      "metadata": {
        "id": "vnf8mmD6zfrG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calculate_accuracy_batches_dataset(X_test, y_test, accuracy, update_op, sess):\n",
        "    sess.run(val_iterator.initializer, feed_dict={val_features_placeholder: X_test,\n",
        "                                                  val_labels_placeholder: y_test})\n",
        "    \n",
        "    try:\n",
        "        while True:\n",
        "            x_batch, y_batch = sess.run([next_val_example, next_val_label])\n",
        "            sess.run(update_op, feed_dict={x_placeholder: x_batch, y_placeholder: y_batch})\n",
        "    except tf.errors.OutOfRangeError:\n",
        "        pass\n",
        "        \n",
        "    return sess.run(accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FfFEYXlZgcMj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the network"
      ]
    },
    {
      "metadata": {
        "id": "tT6rs7hezfuc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epoch_num = 10\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(running_vars_initializer)\n",
        "    \n",
        "    validation_accuracy = calculate_accuracy_batches_dataset(X_test, y_test, accuracy, update_op, sess)\n",
        "    print('Validation accuracy before training: {}'.format(validation_accuracy))\n",
        "    \n",
        "    for epoch in range(epoch_num):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        sess.run(train_iterator.initializer, feed_dict={train_features_placeholder: X_train,\n",
        "                                                        train_labels_placeholder: y_train})\n",
        "        \n",
        "        try:\n",
        "            while True:\n",
        "                x_batch, y_batch = sess.run([next_train_example, next_train_label])\n",
        "                train_step.run(feed_dict={x_placeholder: x_batch, y_placeholder: y_batch})\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "        \n",
        "        validation_accuracy = calculate_accuracy_batches_dataset(X_test, y_test, accuracy, update_op, sess)\n",
        "        end_time = time.time()\n",
        "        print('step: {}, validation accuracy: {}, epoch time: {}'.format(epoch, validation_accuracy, end_time-start_time))\n",
        "        \n",
        "    # Print the test set accuracy\n",
        "    test_accuracy = calculate_accuracy_batches_dataset(X_test, y_test, accuracy, update_op, sess)\n",
        "    print('test accuracy: {}'.format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wJlGDN3j_Jz6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PuGB2AxRggk7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Compare with training without dataset pipeline"
      ]
    },
    {
      "metadata": {
        "id": "gZavL1SSRUIc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compare the epoch times of training with augmentation with using tf dataset and without using tf dataset"
      ]
    },
    {
      "metadata": {
        "id": "-fEC4dGt-gZZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cpu_augmentation_fn_v2(image):\n",
        "    with tf.device('/cpu:0'):\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "        image = tf.image.random_flip_up_down(image)\n",
        "        image = tf.cond(\n",
        "            tf.less(tf.random_uniform(()), 0.5),\n",
        "            lambda: tf.contrib.image.rotate(\n",
        "                image,\n",
        "                tf.random_uniform((), -30 / 180 * math.pi, 30 / 180 * math.pi)\n",
        "            ),\n",
        "            lambda: tf.identity(image)\n",
        "        )\n",
        "\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UWgtFzg_9Vco",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### 1746 seconds per epoch !!!\n",
        "epoch_num = 1\n",
        "batch_size = 128\n",
        "set_size = X_train.shape[0]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(running_vars_initializer)\n",
        "    \n",
        "    validation_accuracy = calculate_accuracy_batches(X_test, y_test, batch_size,\n",
        "                                                     accuracy, update_op, sess)\n",
        "    print('Validation accuracy before training: {}'.format(validation_accuracy))\n",
        "    \n",
        "    for epoch in range(epoch_num):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        perm = np.random.permutation(set_size)\n",
        "        X_train = X_train[perm, :]\n",
        "        y_train = y_train[perm, :]\n",
        "\n",
        "        for i in range(0, set_size, batch_size):\n",
        "            step_size = min(batch_size, set_size - i)\n",
        "\n",
        "            if step_size > 1:\n",
        "                y_batch = y_train[i:(i + step_size), :]\n",
        "                x_batch = X_train[i:(i + step_size), :]\n",
        "                x_batch = tf.map_fn(cpu_augmentation_fn_v2, x_batch)\n",
        "                train_step.run(feed_dict={\n",
        "                                x_placeholder: x_batch.eval(), y_placeholder: y_batch})\n",
        "                \n",
        "        validation_accuracy = calculate_accuracy_batches(X_test, y_test, \n",
        "                                          batch_size, accuracy, update_op, sess)\n",
        "        end_time = time.time()\n",
        "        print('step: {}, validation accuracy: {}, epoch time: {}'.format(epoch, validation_accuracy, end_time-start_time))\n",
        "\n",
        "\n",
        "    # Print the test set accuracy\n",
        "    test_accuracy = calculate_accuracy_batches(X_test, y_test, batch_size, accuracy, update_op, sess)\n",
        "    print('test accuracy: {}'.format(test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jz5f7osajbEY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xXNQIW0VjbJ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IUFnYKYJjc79",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Images sources\n",
        "\n",
        "Images used in this notebook comes from the following web pages:\n",
        "1.   https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/\n",
        "2.   https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
        "3.   https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1\n",
        "4.   https://dominikschmidt.xyz/tensorflow-data-pipeline/\n",
        "5.   https://www.researchgate.net/figure/Effect-of-three-different-edge-detection-filters-Laplacian-Canny-and-Sobel-filters_fig5_236125496\n",
        "\n",
        "\n",
        "# Other references\n",
        "\n",
        "Text is also inspired by:\n",
        "1.   [TensorFlow guide](https://www.tensorflow.org/guide)\n",
        "2.   [DL with TF](https://github.com/ageron/tf2_course) course by Aurelien Geron\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6tYQsReejbH1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N5OPtjU1_bqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}