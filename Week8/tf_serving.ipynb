{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwZsn9nejH5a"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qds8jeMDjhbk"
   },
   "outputs": [],
   "source": [
    "assert sys.version_info >= (3, 5) # Python ≥3.5 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ebziY8gtjh-i",
    "outputId": "a1fd7e66-a184-4904-8400-14846467a03c"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bnRHSBAvjiAt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download fashion-MNIST data\n",
    "\n",
    "And prepare train, valid, test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "2Crqun1cjiDE",
    "outputId": "251d2790-b387-498c-f648-8d080ccda123"
   },
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.\n",
    "X_test = X_test / 255.\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxMLz30DjiE6"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "X_valid = X_valid.reshape(-1,28,28,1)\n",
    "X_test = X_test.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vKkqmyr-jiHL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and train the convolutional neural network for images classification\n",
    "\n",
    "I define the small model as I don't have a GPU on my laptop and moreover test accuracy is not the issue in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8htrp33fl3Tn"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(8, kernel_size=3, activation='relu', padding='same', input_shape=(28,28,1)),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(16, kernel_size=3, activation='relu', padding='same'),\n",
    "    keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "    keras.layers.Conv2D(16, kernel_size=3, activation='relu', padding='same'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "AkNUsatDmmIH",
    "outputId": "2c4b4764-f429-4c65-e947-c4cd8162eba1"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "jJZuU45pl3WS",
    "outputId": "b63b776c-09b2-43c0-82c8-af5c2673a8fd"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rgc-cJkIl3YT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "\n",
    "We trained our model and now we want to use it with TensorFlow serving. However before running the server, we have to save our model.\n",
    "\n",
    "As we can use multiple model architecuters and train the same architecture multiple times, we have to name our model with its unique model version. However, newer models should have bigger versions numbers, as tf server by default runs the model with highest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "85KWH5TQl3ad"
   },
   "outputs": [],
   "source": [
    "all_models_path = 'models'\n",
    "MODEL_NAME = \"fashion_mnist_conv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can name your model with current timestamp. Then you will be sure, that newest version has the highest version number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VnDYnadql3c4"
   },
   "outputs": [],
   "source": [
    "model_version = ###\n",
    "model_path = os.path.join(all_models_path, MODEL_NAME, str(model_version))\n",
    "os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XrMucetVl3fZ",
    "outputId": "17747f98-f32f-4e9e-a7c7-3445f2f627f7"
   },
   "outputs": [],
   "source": [
    "model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tf 2.0 there is an easy way to [save](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/saved_model/save) the tf.keras.model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-kpYxrZl3iL"
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLI to inspect and execute SavedModel\n",
    "\n",
    "You can use the [SavedModel Command Line Interface (CLI)](https://www.tensorflow.org/guide/saved_model#cli_to_inspect_and_execute_savedmodel) to inspect and execute a SavedModel. For example, you can use the CLI to inspect the model's SignatureDefs. The CLI enables you to quickly confirm that the input Tensor dtype and shape match the model. Moreover, if you want to test your model, you can use the CLI to do a sanity check by passing in sample inputs in various formats (for example, Python expressions) and then fetching the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of commands\n",
    "\n",
    "The SavedModel CLI supports the following two commands on a MetaGraphDef in a SavedModel:\n",
    "\n",
    " - show, which shows a computation on a MetaGraphDef in a SavedModel.\n",
    " - run, which runs a computation on a MetaGraphDef.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show command\n",
    "\n",
    "A SavedModel contains one or more MetaGraphDefs, identified by their tag-sets. To serve a model, you might wonder what kind of SignatureDefs are in each model, and what are their inputs and outputs. The show command let you examine the contents of the SavedModel in hierarchical order. Here's the syntax:\n",
    "\n",
    "```bash\n",
    "saved_model_cli show [-h] --dir DIR [--all] [--tag_set TAG_SET] [--signature_def SIGNATURE_DEF_KEY]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try different saved_model_cli formulas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "7BJIfGuDl3km",
    "outputId": "f9ec16b0-4382-47bd-9795-625c2218549d"
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "M8rM70RVoKQV",
    "outputId": "67cd097b-53f0-43e1-b338-177323d942fe"
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "b2Z4fBJmoKS9",
    "outputId": "3ecf1e3a-d4b7-4a55-a90b-098e686732a3"
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "colab_type": "code",
    "id": "3aCGtTdkoKXr",
    "outputId": "62ef1688-88d6-4ff2-bbe2-737a0cd5dd2a"
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uMciTyBgoKVx"
   },
   "source": [
    "### run command\n",
    "\n",
    "Invoke the run command to run a graph computation, passing inputs and then displaying (and optionally saving) the outputs. Here's the syntax:\n",
    "\n",
    "```bash\n",
    "saved_model_cli run [-h] --dir DIR --tag_set TAG_SET --signature_def\n",
    "                           SIGNATURE_DEF_KEY [--inputs INPUTS]\n",
    "                           [--input_exprs INPUT_EXPRS]\n",
    "                           [--input_examples INPUT_EXAMPLES] [--outdir OUTDIR]\n",
    "                           [--overwrite] [--tf_debug]\n",
    "```\n",
    "\n",
    "The run command provides the following three ways to pass inputs to the model:\n",
    "\n",
    " - *inputs* option enables you to pass numpy ndarray in files.\n",
    " - *input_exprs* option enables you to pass Python expressions.\n",
    " - *input_examples* option enables you to pass tf.train.Example.\n",
    "\n",
    "Here we will use the *inputs* option.\n",
    "\n",
    "To pass input data in files, specify the --inputs option, which takes the following general format:\n",
    "\n",
    "```bash\n",
    "--inputs <input_key>=<filename>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input layer name**\n",
    "\n",
    "In order to pass the testing data to our trained model, we have to know the name of its input layer and pass it to *saved_model_cli* as *input_key*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M7eun6sIl3nK",
    "outputId": "dcb39e65-d5d6-4bb3-f614-2022881f2e88"
   },
   "outputs": [],
   "source": [
    "input_name = ###\n",
    "input_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare small testing dataset**\n",
    "\n",
    "We want to test our model. Take 3 images from the tesing dataset, and [save it](https://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html) as *saved_model_cli* takes the *filename* as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3mHssGqoKNB"
   },
   "outputs": [],
   "source": [
    "X_query = ###\n",
    "y_query = ###\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**saved_model_cli run**\n",
    "\n",
    "Specify arguments and run testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "yk6OxEukl3pn",
    "outputId": "f9bf4149-5e30-4646-ddd4-eb56bd48f735"
   },
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "51Ev-pBFR4d3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare docker server with our trained model\n",
    "\n",
    "To this end, one of the easiest ways to serve machine learning models is by using TensorFlow Serving with Docker. Docker is a tool that packages software into units called containers that include everything needed to run the software.\n",
    "\n",
    "In the following subsection we will prepare the docker image that serves our model and try to get the classifications for testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SkZRHxiSR4rH"
   },
   "source": [
    "First, we have to run the docker with the proper image. We can do it in two steps.\n",
    "\n",
    "\n",
    "1. Download the docker image\n",
    "```bash\n",
    "sudo docker pull tensorflow/serving\n",
    "```\n",
    "\n",
    "2. Run the image\n",
    "```bash\n",
    "sudo docker run -it --rm -p 8501:8501 \\\n",
    "   -v \"`pwd`/models/fashion_mnist_conv:/models/fashion_mnist_conv\" \\\n",
    "   -e MODEL_NAME=fashion_mnist_conv \\\n",
    "   tensorflow/serving\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REST API\n",
    "\n",
    "TensorFlow ModelServer also supports [RESTful APIs](https://www.tensorflow.org/tfx/serving/api_rest).\n",
    "\n",
    "The request and response is a JSON object. The composition of this object depends on the request type or verb. \n",
    "\n",
    "Below we will show how to use REST API, together with tf serving, and then make an example client that sends the test image to docker and gets the classification answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uJUDZQ8QR4tc"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Model status API](https://www.tensorflow.org/tfx/serving/api_rest#model_status_api)\n",
    "\n",
    "This API returns the status of a model in the ModelServer.\n",
    "\n",
    "\n",
    "```bash\n",
    "GET http://host:port/v1/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]\n",
    "```\n",
    "\n",
    "*/versions/${MODEL_VERSION}* is optional. If omitted status for **all** versions is returned in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER_URL = ###\n",
    "\n",
    "response = requests.get(SERVER_URL)\n",
    "response.raise_for_status()\n",
    "response = response.json()\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Model Metadata API](https://www.tensorflow.org/tfx/serving/api_rest#model_metadata_api)\n",
    "\n",
    "This API returns the metadata of a model in the ModelServer.\n",
    "\n",
    "```bash\n",
    "GET http://host:port/v1/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/metadata\n",
    "```\n",
    "\n",
    "*/versions/${MODEL_VERSION}* is optional. If omitted the model metadata for the **latest** version is returned in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER_URL = ###\n",
    "\n",
    "response = requests.get(SERVER_URL)\n",
    "response.raise_for_status()\n",
    "response = response.json()\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Predict API](https://www.tensorflow.org/tfx/serving/api_rest#predict_api)\n",
    "\n",
    "This API closely follows the PredictionService.Predict gRPC API.\n",
    "\n",
    "```bash\n",
    "POST http://host:port/v1/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]:predict\n",
    "```\n",
    "\n",
    "*/versions/${MODEL_VERSION}* is optional. If omitted the **latest** version is used.\n",
    "\n",
    "\n",
    "**Request format**\n",
    "\n",
    "The request body for predict API must be JSON object formatted as follows:\n",
    "\n",
    "```python\n",
    "{\n",
    "  // (Optional) Serving signature to use.\n",
    "  // If unspecifed default serving signature is used.\n",
    "  \"signature_name\": <string>,\n",
    "\n",
    "  // Input Tensors in row (\"instances\") or columnar (\"inputs\") format.\n",
    "  // A request can have either of them but NOT both.\n",
    "  \"instances\": <value>|<(nested)list>|<list-of-objects>\n",
    "  \"inputs\": <value>|<(nested)list>|<object>\n",
    "}\n",
    "```\n",
    "\n",
    "**Examples**\n",
    "\n",
    "1. Row representation\n",
    "\n",
    "```python\n",
    "{\n",
    " \"instances\": [\n",
    "   {\n",
    "     \"tag\": \"foo\",\n",
    "     \"signal\": [1, 2, 3, 4, 5],\n",
    "     \"sensor\": [[1, 2], [3, 4]]\n",
    "   },\n",
    "   {\n",
    "     \"tag\": \"bar\",\n",
    "     \"signal\": [3, 4, 1, 2, 5],\n",
    "     \"sensor\": [[4, 5], [6, 8]]\n",
    "   }\n",
    " ]\n",
    "}\n",
    "```\n",
    "\n",
    "2. Columnar representation\n",
    "\n",
    "```python\n",
    "{\n",
    " \"inputs\": {\n",
    "   \"tag\": [\"foo\", \"bar\"],\n",
    "   \"signal\": [[1, 2, 3, 4, 5], [3, 4, 1, 2, 5]],\n",
    "   \"sensor\": [[[1, 2], [3, 4]], [[4, 5], [6, 8]]]\n",
    " }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare the json with input data**\n",
    "\n",
    "We already created some small array with 3 test images. Pass them to json (in representation that you prefer) and post this json to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRJlv3HzR4ka"
   },
   "outputs": [],
   "source": [
    "input_data_json = ###\n",
    "print(input_data_json[:200] + \"...\" + input_data_json[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "maOjrRnnR4gl"
   },
   "outputs": [],
   "source": [
    "SERVER_URL = ###\n",
    "            \n",
    "response = requests.post(SERVER_URL, data=input_data_json)\n",
    "response.raise_for_status()\n",
    "response = response.json()\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YsKtTxAQl3sA"
   },
   "outputs": [],
   "source": [
    "y_proba = np.array(response[\"predictions\"])\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_proba, axis=-1), y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the function that queries the server for the whole testing dataset and returns the network accuracy\n",
    "\n",
    "And compare it with test accuracy that we computed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_for_answers(X_test, SERVER_URL, batch_size=16):\n",
    "    ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_for_answers(X_test, SERVER_URL, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
